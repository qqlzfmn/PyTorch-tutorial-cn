{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络\n",
    "\n",
    "可以使用 `torch.nn` 包构建神经网络。\n",
    "\n",
    "现在你已经对 `autograd` 有了一定了解，而 `nn` 是依赖于 `autograd` 来定义模型并区分的。\n",
    "一个 `nn.Module` 包括 layers 和一个返回 `output` 的方法 `forward(input)` 。\n",
    "\n",
    "例如下面这个用于分类数字图像的网络：\n",
    "\n",
    "![convnet](./img/mnist.png)\n",
    "\n",
    "这是一个简单的前馈网络。它接收输入并馈送到网络中，输入经过一层一层的网络之后最终得到输出。\n",
    "\n",
    "一个神经网络的典型训练过程如下：\n",
    "\n",
    "- 定义有一些可学习参数（或权重）的神经网络\n",
    "- 在输入数据集上进行迭代\n",
    "- 让输入通过网络\n",
    "- 计算损失（loss ，其实就是输出与标准值 label 之间的差距）\n",
    "- 将梯度反向传播给网络参数\n",
    "- 更新网络权重，通常使用一个简单的更新规则：\n",
    "  `weight = weight - learning_rate * gradient`\n",
    "\n",
    "## 定义网络\n",
    "\n",
    "让我们来定义这个网络吧：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 个输入图像通道， 6 个输出通道， 5 x 5 见方的卷积核\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # 一个仿射变换： y = Wx + b\n",
    "        # 仿射变换，又称仿射映射，是指在几何中，一个向量空间进行一次线性变换并接上一个平移，变换为另一个向量空间。\n",
    "        # 一个对向量 x 平移 b ， 与旋转放大缩小 W 的仿射变换为 y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 在一个 2 x 2 的窗上进行最大池化\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # 如果 size 是和正方形，则只能指定一个数字\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # 除批次维度外的所有维度\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你只需要定义 `forward` 函数，然后（计算梯度的） `backward` 函数就会使用 `autograd` 为你自动定义好。\n",
    "你可以在 `forward` 函数中使用任何 Tensor 的操作。\n",
    "\n",
    "模型的可学习参数由 `net.parameters()` 返回。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1 的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们用一个随机的 32 x 32 的输入来试一下。\n",
    "注意：这个网络（LeNet）的预期输入大小是 32 x 32 的。如果你想在 MNIST 数据集上使用这个网络，请把图片大小 resize 到 32 x 32 。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将所有参数的梯度缓存清零并使用随机梯度进行反向传播：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**注意**  \n",
    ">`torch.nn` 只支持小批量。整个 `torch.nn` 包只支持小批量样本的输入，并且不能是单个样本。  \n",
    ">例如 `nn.Conv2d` 在 `nSamples x nChannels x Height x Width` 的 4D Tensor 中执行。  \n",
    ">如果你只有一个样本，你需要使用 `input.unsqueeze(0)` 来生成一个假的批量维度。\n",
    "\n",
    "在继续之前，让我们回顾一下之前你见过的所有类。\n",
    "\n",
    "**回顾：**\n",
    "- `torch.Tensor` - 一个具有像 `backward()` 这种 autograd 操作支持的*多维数组*。并且存有*关于 tensor 的梯度*。\n",
    "- `nn.Module` - 神经网络模块。*方便的封装参数的方法*，具有移动到 GPU 、导出、加载等功能。\n",
    "- `nn.Parameter` - 一种 Tensor ，*当被指派为 `Module` 的属性时，自动注册为 `Module` 的一个参数*。\n",
    "- `autograd.Function` - 实现了 *autograd 操作的前向和后向的定义*。每一个 `Tensor` 操作创建至少一个 `Function` 节点，该节点连接到创建 `Tensor` 并*编码了它自己历史*的 `Function` 上。\n",
    "\n",
    "**至此，我们已经讨论过：**\n",
    "- 定义一个神经网络\n",
    "- 网络如何处理输入以及调用 backward\n",
    "\n",
    "**尚未讨论：**\n",
    "- 计算损失\n",
    "- 更新网络权重\n",
    "\n",
    "## 损失函数\n",
    "损失函数以 (output, target) 参数对作为输入，计算 output 和 target 之间距离的估值。\n",
    "\n",
    "在 nn 包下有几种不同的[损失函数](https://pytorch.org/docs/nn.html#loss-functions)。\n",
    "一个简单的损失函数是： `nn.MSELoss` ，它计算 output 和 target 之间的均方差。\n",
    "\n",
    "下面是例子：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # 例如一个虚拟的 target\n",
    "target = target.view(1, -1)  # 使它与 output 形状相同\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在如果你使用 `.grad_fn` 属性反向跟踪 `loss` ，你将看到一个如下的计算图：\n",
    "\n",
    "```\n",
    "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "          -> view -> linear -> relu -> linear -> relu -> linear\n",
    "          -> MSELoss\n",
    "          -> loss\n",
    "```\n",
    "\n",
    "所以，当我们调用 `loss.backward()` 的时候，整个图将根据损失求微分，所有在图中有 `requires_grad=True` 的 Tensor 都将有一个 `.grad` Tensor 累积到梯度上。\n",
    "\n",
    "为了详细说明，让我们反向走几步：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播\n",
    "想要反向传播误差，我们只需要 `loss.backward()` 。\n",
    "你需要清理之前存在的梯度，否则新的梯度将累加到之前存在的梯度上。\n",
    "\n",
    "现在我们将调用 `loss.backward()` ，然后查看 conv1 的 bias 的梯度在 backward 前后的变化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.zero_grad()     # 将所有参数的梯度缓存清零\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经知道如何使用损失函数了。\n",
    "\n",
    "**稍后阅读：**\n",
    "\n",
    "　　神经网络软件包包括构建深度神经网络的各种模块和损失函数。[这里](https://pytorch.org/docs/nn)有完整的文档列表。\n",
    "\n",
    "**还剩最后一步要学：**\n",
    "\n",
    "- 更新神经网络的权重\n",
    "\n",
    "## 更新权重\n",
    "实践中使用的最简单的更新规则是随机梯度下降（SGD）：\n",
    "\n",
    "`weight = weight - learning_rate * gradient`\n",
    "\n",
    "我们可以用简单的 Python 代码实现：\n",
    "\n",
    "```python\n",
    "    learning_rate = 0.01\n",
    "    for f in net.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)\n",
    "```\n",
    "\n",
    "然而，当你使用神经网络的时候，你想要使用各种不用的更新规则，比如 SGD 、 Nesterov-SGD 、 Adam 、 RMSProp 等等。\n",
    "为了使这变为可能，我们构建了一个小的软件包： `torch.optim` 来实现所有这些方法。它的使用非常简单：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 构建你的优化器\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# 在你的训练循环过程中：\n",
    "optimizer.zero_grad()   # 将梯度缓存清零\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # 更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">注意  \n",
    ">观察为何梯度缓存必须使用 `optimizer.zero_grad()` 手动设置为 0 。\n",
    ">这是因为在“反向传播”部分中解释过的：梯度是累加的。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
