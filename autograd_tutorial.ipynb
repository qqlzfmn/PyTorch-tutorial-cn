{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd: 自动微分\n",
    "\n",
    "PyTorch 中所有神经网络的核心是 `autograd` 软件包。\n",
    "让我们先简单了解一下这个包，然后去训练我们的第一个神经网络。\n",
    "\n",
    "`autograd` 软件包为 Tensor 上的所有操作提供自动微分。\n",
    "它是一个 define-by-run 的框架，这意味着你的反向传播由你的代码的运行方式定义，并且每次迭代都可以不同。\n",
    "\n",
    "让我们用更简单的描述和一些例子来了解这一特性吧。\n",
    "\n",
    "## Tensor\n",
    "\n",
    "`torch.Tensor` 是 PyTorch 软件包的核心类。\n",
    "如果你把 Tensor 的属性 `.requires_grad` 设置为 `True`，它就会开始跟踪所有在它身上发生的操作。\n",
    "当你结束计算的时候，你可以调用 `.backward()` 方法，这时所有的梯度都会自动计算好。\n",
    "这个 Tensor 的梯度将会累积到它的 `.grad` 属性中。\n",
    "\n",
    "想要阻止一个 Tensor 跟踪历史，你可以调用 `.detach()` 方法将它从计算历史中分离出来，并阻止它在以后的计算中被跟踪。\n",
    "\n",
    "要阻止跟踪历史（和使用内存），你还可以使用 `with torch.no_grad():` 包裹代码块。\n",
    "这可能在 evaluate 一个模型的时候非常管用，因为模型可能含有不需要计算梯度的包含 `requires_grad=True` 属性的可训练参数。\n",
    "\n",
    "还有一个对于 autograd 的实现非常重要的类 - `Function` 。\n",
    "\n",
    "`Tensor` 和 `Function` 互相连接并构建一个编码了完整计算历史的无环图。\n",
    "每一个 Tensor 都有一个 `.grad_fn` 属性，该属性包含创建这个 `Tensor` 的 `Function` 的引用（除了 user 创建的 Tensor ，因为它们的 `grad_fn` 是 `None` ）。\n",
    "\n",
    "如果你想要计算导数，你可以在一个 `Tensor` 上调用 `.backward()` 方法。\n",
    "如果 `Tensor` 是一个标量（即它只有一个数据元素），你不需要为 `.backward()` 指定任何参数，但是如果它有更多元素，你需要指定一个形状匹配的张量作为 `gradient` 参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个 Tensor 并设置属性 `requires_grad=True` 以跟踪计算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行 Tensor 操作：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y` 是一个操作的结果，所以它具有 `grad_fn` 属性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 `y` 上进行更多操作\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.requires_grad_( ... )` 更改一个现有的 Tensor 的 `requires_grad` 属性。\n",
    "如果没有给出， `requires_grad` 属性的默认值是 `False` 。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度（Gradients）\n",
    "现在让我们进行反向传播吧。因为 `out` 只包含一个标量，所以 `out.backward()` 等价于 `out.backward(torch.tensor(1.))` 。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印梯度 $d(out) / dx$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你应该得到了一个全是 `4.5` 的矩阵。我们先把 `out` 记作 *Tensor* “$o$” 。\n",
    "\n",
    "我们已经知道 $o = \\frac{1}{4}\\sum_i z_i$ ， $z_i = 3(x_i+2)^2$ 和 $z_i\\bigr\\rvert_{x_i=1} = 27$ 。\n",
    "\n",
    "所以 $\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$ ，\n",
    "\n",
    "因此 $\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$ 。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在数学上，如果你有一个向量值函数 $\\vec{y}=f(\\vec{x})$ ，那么 $\\vec{y}$ 关于 $\\vec{x}$ 的梯度是一个雅可比矩阵（Jacobian matrix）：\n",
    "\n",
    "\\begin{align}J=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\end{align}\n",
    "\n",
    "一般来说， `torch.autograd` 是一个计算雅可比向量积的引擎。\n",
    "也就是说，给定任意向量$v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}$ ， 计算向量积 $J\\cdot v$ 。 \n",
    "如果 $v$ 恰好是标量函数 $l=g\\left(\\vec{y}\\right)$ 的梯度，也就是说， $v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$ ，那么根据链式法则，雅可比向量积将等于 $l$ 关于 $\\vec{x}$ 的梯度：\n",
    "\n",
    "\\begin{align}J\\cdot v=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\left(\\begin{array}{c}\n",
    "   \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
    "   \\vdots\\\\\n",
    "   \\frac{\\partial l}{\\partial y_{m}}\n",
    "   \\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "   \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
    "   \\vdots\\\\\n",
    "   \\frac{\\partial l}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\end{align}\n",
    "\n",
    "雅可比向量积的这一特性使得将外部梯度馈送到一个具有非标量输出的模型中非常方便。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在让我们看一个雅可比向量积的例子吧：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个实例中， `y` 不再是一个标量了。 `torch.autograd` 无法直接计算完整的雅可比矩阵，但是如果我们只想得到雅可比向量积，则只需要将向量作为参数传递给 `.backward()` 方法：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你也可以使用 `with torch.no_grad():` 包裹代码块来停止 autograd 跟踪在属性为 `.requires_grad=True` 的 Tensor 上的历史：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "\tprint((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**稍后阅读：**\n",
    "\n",
    "　　``autograd`` 和 ``Function`` 的文档在 https://pytorch.org/docs/autograd\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
